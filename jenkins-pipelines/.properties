SOURCES_DIR_TRAIN=diabetes_regression
# The path to the model training script under SOURCES_DIR_TRAIN
TRAIN_SCRIPT_PATH=training/train_aml.py
# The path to the model evaluation script under SOURCES_DIR_TRAIN
EVALUATE_SCRIPT_PATH=evaluate/evaluate_model.py
# The path to the model registration script under SOURCES_DIR_TRAIN
REGISTER_SCRIPT_PATH=register/register_model.py
# The path to the model scoring script relative to SOURCES_DIR_TRAIN
SCORE_SCRIPT=scoring/score.py


# Azure ML Variables
EXPERIMENT_NAME=mlopsjenkins
DATASET_NAME=diabetes_ds
# Uncomment DATASTORE_NAME if you have configured non default datastore to point to your data
# DATASTORE_NAME=datablobstore
DATASET_VERSION=latest
TRAINING_PIPELINE_NAME=diabetes-training-pipeline
MODEL_NAME=diabetes_regression_model.pkl

# AML Compute Cluster Config
AML_ENV_NAME=diabetes_regression_training_env
AML_ENV_TRAIN_CONDA_DEP_FILE=conda_dependencies.yml
AML_COMPUTE_CLUSTER_CPU_SKU=STANDARD_DS2_V2
AML_COMPUTE_CLUSTER_NAME=train-cluster
AML_CLUSTER_MIN_NODES=0
AML_CLUSTER_MAX_NODES=4
AML_CLUSTER_PRIORITY=lowpriority

# The name for the (docker/webapp) scoring image
IMAGE_NAME="diabetestrained"

# Optional. Used by a training pipeline with R on Databricks

# These are the default values set in ml_service\util\env_variables.py. Uncomment and override if desired.
# Set to false to disable the evaluation step in the ML pipeline and register the newly trained model unconditionally.
#  RUN_EVALUATION=true
# Set to false to register the model regardless of the outcome of the evaluation step in the ML pipeline.
#  ALLOW_RUN_CANCEL=true

# Flag to allow rebuilding the AML Environment after it was built for the first time. This enables dependency updates from conda_dependencies.yaml.
AML_REBUILD_ENVIRONMENT=true

# Variables below are used for controlling various aspects of batch scoring
USE_GPU_FOR_SCORING=False
# Conda dependencies for the batch scoring step
AML_ENV_SCORE_CONDA_DEP_FILE=conda_dependencies_scoring.yml
# Conda dependencies for the score copying step
AML_ENV_SCORECOPY_CONDA_DEP_FILE=conda_dependencies_scorecopy.yml
# AML Compute Cluster Config for parallel batch scoring
AML_ENV_NAME_SCORING=diabetes_regression_scoring_env
AML_ENV_NAME_SCORE_COPY=diabetes_regression_score_copy_env
AML_COMPUTE_CLUSTER_CPU_SKU_SCORING=STANDARD_DS2_V2
AML_COMPUTE_CLUSTER_NAME_SCORING=score-cluster
AML_CLUSTER_MIN_NODES_SCORING=0
AML_CLUSTER_MAX_NODES_SCORING=4
AML_CLUSTER_PRIORITY_SCORING=lowpriority
# The path to the batch scoring script relative to SOURCES_DIR_TRAIN
BATCHSCORE_SCRIPT_PATH=scoring/parallel_batchscore.py
BATCHSCORE_COPY_SCRIPT_PATH=scoring/parallel_batchscore_copyoutput.py
# Flag to allow rebuilding the AML Environment after it was built for the first time.
# This enables dependency updates from the conda dependencies yaml for scoring activities.
AML_REBUILD_ENVIRONMENT_SCORING="true"

# Datastore config for scoring
# The storage account name and key are supplied as variables in a variable group
# in the Azure Pipelines library for this project. Please refer to repo docs for
# more details

# Blob container where the input data for scoring can be found
SCORING_DATASTORE_INPUT_CONTAINER="input"
# Blobname for the input data - include any applicable path in the string
SCORING_DATASTORE_INPUT_FILENAME="diabetes_scoring_input.csv"
# Blob container where the output data for scoring can be found
SCORING_DATASTORE_OUTPUT_CONTAINER="output"
# Blobname for the output data - include any applicable path in the string
SCORING_DATASTORE_OUTPUT_FILENAME="diabetes_scoring_output.csv"
# Dataset name for input data for scoring
SCORING_DATASET_NAME="diabetes_scoring_ds"
# Scoring pipeline name
SCORING_PIPELINE_NAME="diabetes-scoring-pipeline"
